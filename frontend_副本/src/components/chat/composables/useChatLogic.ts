import { computed, watch } from 'vue'
import { usePromptStore } from '@/stores/promptStore'
import { useSettingsStore } from '@/stores/settingsStore'
import { useNotificationStore } from '@/stores/notificationStore'
import { AIGuideService } from '@/services/aiGuideService'
import { AIService } from '@/services/aiService'
import { PromptGeneratorService } from '@/services/promptGeneratorService'
import { getPromptGeneratorConfig } from '@/config/promptGenerator'
import { cleanAIResponse, checkAIDecision } from '@/utils/aiResponseUtils'

export function useChatLogic(
  chatMessages: { 
    startStreamingMessage: () => number
    updateStreamingMessage: (content: string) => void
    simulateTyping: (message: string, isStreaming: boolean) => Promise<void>
    scrollToBottom: () => void
  },
  chatModel: {
    getCurrentChatModel: () => any
    isStreamMode: { value: boolean }
    showModelSelector: { value: boolean }
  },
  chatInput: {
    clearInput: () => void
  },
  chatAttachments: {
    currentAttachments: { value: any[] }
    clearAttachments: () => void
  },
  chatQuickReplies: {
    checkForceGenerate: (input: string) => boolean
    showQuickReplies: { value: boolean }
  }
) {
  const config = getPromptGeneratorConfig()
  const promptStore = usePromptStore()
  const settingsStore = useSettingsStore()
  const notificationStore = useNotificationStore()
  const aiGuideService = AIGuideService.getInstance()

  const chatContainerMaxHeight = computed(() => {
    // åŸºç¡€é«˜åº¦ï¼šçº¦345px
    let baseCalculation = 345
    
    let modelSelectorExtraHeight = 0
    
    if (chatModel.showModelSelector.value) {
      if (typeof window !== 'undefined' && window.innerWidth >= 640) {
        modelSelectorExtraHeight = 110
      } else {
        modelSelectorExtraHeight = 120
      }
    }
    
    // è®¡ç®—é™„ä»¶åŒºåŸŸé¢å¤–é«˜åº¦
    // æ€»è®¡çº¦ 115px
    let attachmentExtraHeight = 0
    if (chatAttachments.currentAttachments.value.length > 0) {
      attachmentExtraHeight = 115
    }
    
    const totalReduction = baseCalculation + modelSelectorExtraHeight + attachmentExtraHeight
    return `calc(100vh - ${totalReduction}px)`
  })

  watch(() => promptStore.chatMessages.length, chatMessages.scrollToBottom)
  watch(() => promptStore.isTyping, chatMessages.scrollToBottom)

  const initializeChat = async () => {
    if (promptStore.chatMessages.length === 0 && !promptStore.isInitialized) {
      promptStore.isInitialized = true
      await chatMessages.simulateTyping(config.welcomeMessage, false)
    }
  }

  const clearChat = () => {
    promptStore.clearChat()
    chatQuickReplies.showQuickReplies.value = false
    chatAttachments.clearAttachments()
    
    setTimeout(async () => {
      await chatMessages.simulateTyping(config.welcomeMessage, false)
      promptStore.isInitialized = true
    }, 500)
  }

  const sendMessage = async (userInput: string) => {
    if (!userInput.trim()) {
      if (chatAttachments.currentAttachments.value.length > 0) {
        notificationStore.warning('è¯·è¾“å…¥æ¶ˆæ¯å†…å®¹ï¼Œä¸èƒ½åªå‘é€é™„ä»¶')
      }
      return
    }
    
    const { provider, model } = chatModel.getCurrentChatModel()
    
    if (!provider || !model) {
      notificationStore.warning('è¯·å…ˆåœ¨å³ä¸Šè§’è®¾ç½®ä¸­é…ç½®AIæ¨¡å‹å’ŒAPIå¯†é’¥')
      return
    }

    promptStore.clearProgressMessages()

    const currentInput = userInput
    const attachments = [...chatAttachments.currentAttachments.value]
    
    const isForceGenerate = chatQuickReplies.checkForceGenerate(currentInput)
    
    promptStore.addMessage('user', currentInput, attachments)
    
    chatInput.clearInput()
    chatAttachments.clearAttachments()
    chatQuickReplies.showQuickReplies.value = false
    
    if (isForceGenerate) {
      await chatMessages.simulateTyping('å¥½çš„ï¼Œæˆ‘å°†ç«‹å³ä¸ºæ‚¨ç”Ÿæˆéœ€æ±‚æŠ¥å‘Šã€‚', false)
      
      setTimeout(async () => {
        const globalProvider = settingsStore.getCurrentProvider()
        const globalModel = settingsStore.getCurrentModel()
        if (globalProvider && globalModel) {
          await generatePrompt(globalProvider, globalModel.id)
        }
      }, 800)
      return
    }

    promptStore.isTyping = true

    try {
      const useStreamMode = chatModel.isStreamMode.value
      
      if (useStreamMode) {
        const aiService = AIService.getInstance()
        
        let streamingContent = ''
        let messageIndex = -1
        
        aiService.setStreamUpdateCallback((chunk: string) => {
          if (messageIndex === -1) {
            messageIndex = chatMessages.startStreamingMessage()
          }
          streamingContent += chunk
          const cleanContent = cleanAIResponse(streamingContent)
          chatMessages.updateStreamingMessage(cleanContent)
          chatMessages.scrollToBottom()
        })
        
        const validMessages = promptStore.getValidMessages()
        const conversationHistory = validMessages.map(msg => ({
          type: msg.type,
          content: msg.content,
          attachments: msg.attachments || []
        }))
        
        const aiResponse = await aiGuideService.generateSimpleResponse(
          '',
          conversationHistory,
          provider,
          model.id,
          useStreamMode
        )

        aiService.clearStreamUpdateCallback()

        if (useStreamMode && messageIndex === -1) {
          messageIndex = chatMessages.startStreamingMessage()
          const cleanContent = cleanAIResponse(aiResponse)
          chatMessages.updateStreamingMessage(cleanContent)
        } else if (useStreamMode && streamingContent.trim() === '') {
          const cleanContent = cleanAIResponse(aiResponse)
          chatMessages.updateStreamingMessage(cleanContent)
        }

        const shouldEndConversation = checkAIDecision(aiResponse)
        
        if (shouldEndConversation || aiResponse.includes('åŸºäºæˆ‘ä»¬çš„å¯¹è¯ï¼Œæˆ‘ç°åœ¨ä¸ºæ‚¨ç”Ÿæˆéœ€æ±‚æŠ¥å‘Šï¼š')) {
          setTimeout(async () => {
            const globalProvider = settingsStore.getCurrentProvider()
            const globalModel = settingsStore.getCurrentModel()
            if (globalProvider && globalModel) {
              await generatePrompt(globalProvider, globalModel.id)
            }
          }, 800)
        }
      } else {
        const validMessages = promptStore.getValidMessages()
        const conversationHistory = validMessages.map(msg => ({
          type: msg.type,
          content: msg.content,
          attachments: msg.attachments || []
        }))
        const aiResponse = await aiGuideService.generateSimpleResponse(
          '',
          conversationHistory,
          provider,
          model.id,
          useStreamMode
        )

        const shouldEndConversation = checkAIDecision(aiResponse)
        
        if (shouldEndConversation || aiResponse.includes('åŸºäºæˆ‘ä»¬çš„å¯¹è¯ï¼Œæˆ‘ç°åœ¨ä¸ºæ‚¨ç”Ÿæˆéœ€æ±‚æŠ¥å‘Šï¼š')) {
          const cleanResponse = cleanAIResponse(aiResponse)
          await chatMessages.simulateTyping(cleanResponse, false)
          
          setTimeout(async () => {
            const globalProvider = settingsStore.getCurrentProvider()
            const globalModel = settingsStore.getCurrentModel()
            if (globalProvider && globalModel) {
              await generatePrompt(globalProvider, globalModel.id)
            }
          }, 800)
        } else {
          const cleanResponse = cleanAIResponse(aiResponse)
          await chatMessages.simulateTyping(cleanResponse, false)
        }
      }
    } catch (error: unknown) {
      promptStore.isTyping = false
      promptStore.isGenerating = false
      const errorMessage = error instanceof Error ? error.message : String(error)
      notificationStore.error(`å‘ç”Ÿé”™è¯¯: ${errorMessage}`)
      
      if (chatModel.isStreamMode.value) {
        const aiService = AIService.getInstance()
        aiService.clearStreamUpdateCallback()
      }
    }
  }

  const generatePrompt = async (provider: any, modelId: string) => {
    try {
      promptStore.clearProgressMessages()

      const validMessages = promptStore.getValidMessages()
      const conversationHistory = validMessages.map(msg => ({
        type: msg.type,
        content: msg.content,
        attachments: msg.attachments || []
      }))
      
      promptStore.isGenerating = true
      promptStore.currentExecutionStep = 'report'
      promptStore.addOrUpdateProgressMessage('ğŸ”„ æ­£åœ¨åŸºäºå¯¹è¯ç”Ÿæˆéœ€æ±‚æŠ¥å‘Š...', 'progress')
      
      promptStore.promptData.requirementReport = ''
      
      const onReportStreamUpdate = (chunk: string) => {
        promptStore.promptData.requirementReport += chunk
      }
      
      const requirementReport = await aiGuideService.generateRequirementReportFromConversation(
        conversationHistory,
        provider,
        modelId,
        settingsStore.streamMode ? onReportStreamUpdate : undefined
      )
      
      // åªåœ¨éæµå¼æ¨¡å¼ä¸‹è¦†ç›–å†…å®¹ï¼ˆæµå¼æ¨¡å¼å·²é€šè¿‡å›è°ƒæ›´æ–°ï¼‰
      if (!settingsStore.streamMode) {
        promptStore.promptData.requirementReport = requirementReport
      }
      promptStore.showPreview = true
      
      if (promptStore.isAutoMode) {
        promptStore.addOrUpdateProgressMessage('âœ… éœ€æ±‚æŠ¥å‘Šå·²ç”Ÿæˆï¼æ­£åœ¨è‡ªåŠ¨æ‰§è¡Œå®Œæ•´çš„æç¤ºè¯ç”Ÿæˆæµç¨‹...', 'progress')
        
        const promptGeneratorService = PromptGeneratorService.getInstance()
        
        promptStore.currentExecutionStep = 'thinking'
        promptStore.addOrUpdateProgressMessage('ğŸ”„ æ­¥éª¤ 1/4: æ­£åœ¨åˆ†æéœ€æ±‚å¹¶ç”Ÿæˆå…³é”®æŒ‡ä»¤...', 'progress')
        
        let step1Content = ''
        const onStep1Update = (chunk: string) => {
          step1Content += chunk
          const points = step1Content.split('\n').map(s => s.replace(/^[*-]\s*/, '').trim()).filter(Boolean)
          if (points.length > 0) {
            promptStore.promptData.thinkingPoints = points
          }
        }
        
        const thinkingPoints = await promptGeneratorService.getSystemPromptThinkingPoints(
          promptStore.promptData.requirementReport || requirementReport,
          modelId,
          'zh',
          [],
          provider,
          settingsStore.streamMode ? onStep1Update : undefined
        )
        
        // åªåœ¨éæµå¼æ¨¡å¼ä¸‹è¦†ç›–å†…å®¹
        if (!settingsStore.streamMode) {
          promptStore.promptData.thinkingPoints = thinkingPoints
        }
        
        promptStore.currentExecutionStep = 'initial'
        promptStore.addOrUpdateProgressMessage('ğŸ”„ æ­¥éª¤ 2/4: æ­£åœ¨åŸºäºå…³é”®æŒ‡ä»¤ç”Ÿæˆåˆå§‹æç¤ºè¯...', 'progress')
        
        promptStore.promptData.initialPrompt = ''
        const onStep2Update = (chunk: string) => {
          promptStore.promptData.initialPrompt += chunk
        }
        
        const initialPrompt = await promptGeneratorService.generateSystemPrompt(
          promptStore.promptData.requirementReport || requirementReport,
          modelId,
          'zh',
          [],
          promptStore.promptData.thinkingPoints || thinkingPoints,
          provider,
          settingsStore.streamMode ? onStep2Update : undefined
        )
        
        // åªåœ¨éæµå¼æ¨¡å¼ä¸‹è¦†ç›–å†…å®¹
        if (!settingsStore.streamMode) {
          promptStore.promptData.initialPrompt = initialPrompt
        }
        
        promptStore.currentExecutionStep = 'advice'
        promptStore.addOrUpdateProgressMessage('ğŸ”„ æ­¥éª¤ 3/4: æ­£åœ¨åˆ†ææç¤ºè¯å¹¶ç”Ÿæˆä¼˜åŒ–å»ºè®®...', 'progress')
        
        let step3Content = ''
        const onStep3Update = (chunk: string) => {
          step3Content += chunk
          const adviceList = step3Content.split('\n').map(s => s.replace(/^[*-]\s*/, '').trim()).filter(Boolean)
          if (adviceList.length > 0) {
            promptStore.promptData.advice = adviceList
          }
        }
        
        const advice = await promptGeneratorService.getOptimizationAdvice(
          promptStore.promptData.initialPrompt || initialPrompt,
          'system',
          modelId,
          'zh',
          [],
          provider,
          settingsStore.streamMode ? onStep3Update : undefined
        )
        
        // åªåœ¨éæµå¼æ¨¡å¼ä¸‹è¦†ç›–å†…å®¹
        if (!settingsStore.streamMode) {
          promptStore.promptData.advice = advice
        }
        
        promptStore.currentExecutionStep = 'final'
        promptStore.addOrUpdateProgressMessage('ğŸ”„ æ­¥éª¤ 4/4: æ­£åœ¨åº”ç”¨ä¼˜åŒ–å»ºè®®ï¼Œç”Ÿæˆæœ€ç»ˆæç¤ºè¯...', 'progress')
        
        promptStore.promptData.generatedPrompt = ''
        const onStep4Update = (chunk: string) => {
          promptStore.promptData.generatedPrompt += chunk
        }
        
        const finalPrompt = await promptGeneratorService.applyOptimizationAdvice(
          promptStore.promptData.initialPrompt || initialPrompt,
          promptStore.promptData.advice || advice,
          'system',
          modelId,
          'zh',
          [],
          provider,
          settingsStore.streamMode ? onStep4Update : undefined
        )
        
        // åªåœ¨éæµå¼æ¨¡å¼ä¸‹è¦†ç›–å†…å®¹
        if (!settingsStore.streamMode) {
          promptStore.promptData.generatedPrompt = finalPrompt
        }
        promptStore.addOrUpdateProgressMessage('âœ… å·²ä¸ºæ‚¨ç”Ÿæˆé«˜è´¨é‡çš„AIæç¤ºè¯ï¼å³ä¾§å¯æŸ¥çœ‹å®Œæ•´çš„ç”Ÿæˆè¿‡ç¨‹å’Œæœ€ç»ˆç»“æœã€‚', 'progress')
        
      } else {
        promptStore.addOrUpdateProgressMessage('âœ… éœ€æ±‚æŠ¥å‘Šå·²ç”Ÿæˆï¼è¯·åœ¨å³ä¾§é¢„è§ˆé¢æ¿ä¸­æŸ¥çœ‹ï¼Œæ‚¨å¯ä»¥æ‰‹åŠ¨æ‰§è¡Œæ¯ä¸ªæ­¥éª¤ã€‚', 'progress')
      }
      
      promptStore.isGenerating = false
      promptStore.currentExecutionStep = null
      
    } catch (error: unknown) {
      promptStore.isGenerating = false
      promptStore.currentExecutionStep = null
      
      const errorMessage = error instanceof Error ? error.message : String(error)
      notificationStore.error(`æç¤ºè¯ç”Ÿæˆå¤±è´¥: ${errorMessage}ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®åé‡è¯•`)
    }
  }

  const regenerateMessage = async (messageId: string, messageIndex: number, provider: any, model: any) => {
    const message = promptStore.chatMessages.find(msg => msg.id === messageId)
    if (!message || message.type !== 'ai') {
      return
    }

    if (!provider || !model) {
      notificationStore.warning('è¯·å…ˆåœ¨å³ä¸Šè§’è®¾ç½®ä¸­é…ç½®AIæ¨¡å‹å’ŒAPIå¯†é’¥')
      return
    }

    try {
      promptStore.clearProgressMessages()
      
      const contextMessages = promptStore.getValidMessages().slice(0, messageIndex)
      const conversationHistory = contextMessages.map(msg => ({
        type: msg.type,
        content: msg.content,
        attachments: msg.attachments || []
      }))
      
      promptStore.isTyping = true
      
      if (chatModel.isStreamMode.value) {
        const aiService = AIService.getInstance()
        
        let streamingContent = ''
        
        aiService.setStreamUpdateCallback((chunk: string) => {
          streamingContent += chunk
          const cleanContent = cleanAIResponse(streamingContent)
          promptStore.updateMessage(messageId, cleanContent)
          chatMessages.scrollToBottom()
        })
        
        const aiResponse = await aiGuideService.generateSimpleResponse(
          '',
          conversationHistory,
          provider,
          model.id,
          true
        )

        aiService.clearStreamUpdateCallback()
        
        const finalContent = cleanAIResponse(aiResponse)
        promptStore.updateMessage(messageId, finalContent)
        
      } else {
        const aiResponse = await aiGuideService.generateSimpleResponse(
          '',
          conversationHistory,
          provider,
          model.id,
          false
        )
        
        const cleanResponse = cleanAIResponse(aiResponse)
        promptStore.updateMessage(messageId, cleanResponse)
      }
      
      promptStore.isTyping = false
      notificationStore.success('æ¶ˆæ¯å·²é‡æ–°ç”Ÿæˆ')
      
    } catch (error: unknown) {
      promptStore.isTyping = false
      const errorMessage = error instanceof Error ? error.message : String(error)
      notificationStore.error(`é‡æ–°ç”Ÿæˆå¤±è´¥: ${errorMessage}`)
      
      if (chatModel.isStreamMode.value) {
        const aiService = AIService.getInstance()
        aiService.clearStreamUpdateCallback()
      }
    }
  }

  const resendUserMessage = async (messageId: string, messageIndex: number, provider: any, model: any) => {
    const message = promptStore.chatMessages.find(msg => msg.id === messageId)
    if (!message || message.type !== 'user') {
      return
    }

    if (!provider || !model) {
      notificationStore.warning('è¯·å…ˆåœ¨å³ä¸Šè§’è®¾ç½®ä¸­é…ç½®AIæ¨¡å‹å’ŒAPIå¯†é’¥')
      return
    }

    try {
      promptStore.clearProgressMessages()
      
      if (messageIndex !== -1) {
        for (let i = messageIndex + 1; i < promptStore.chatMessages.length; i++) {
          const msg = promptStore.chatMessages[i]
          if (msg && !msg.isProgress) {
            promptStore.deleteMessage(msg.id!)
          }
        }
      }

      promptStore.isTyping = true

      const useStreamMode = chatModel.isStreamMode.value
      
      if (useStreamMode) {
        const aiService = AIService.getInstance()
        
        let streamingContent = ''
        let msgIndex = -1
        
        aiService.setStreamUpdateCallback((chunk: string) => {
          if (msgIndex === -1) {
            msgIndex = chatMessages.startStreamingMessage()
          }
          streamingContent += chunk
          const cleanContent = cleanAIResponse(streamingContent)
          chatMessages.updateStreamingMessage(cleanContent)
          chatMessages.scrollToBottom()
        })
        
        const validMessages = promptStore.getValidMessages()
        const conversationHistory = validMessages.map(msg => ({
          type: msg.type,
          content: msg.content,
          attachments: msg.attachments || []
        }))
        const aiResponse = await aiGuideService.generateSimpleResponse(
          '',
          conversationHistory,
          provider,
          model.id,
          useStreamMode
        )

        aiService.clearStreamUpdateCallback()

        const shouldEndConversation = checkAIDecision(aiResponse)
        
        if (shouldEndConversation || aiResponse.includes('åŸºäºæˆ‘ä»¬çš„å¯¹è¯ï¼Œæˆ‘ç°åœ¨ä¸ºæ‚¨ç”Ÿæˆéœ€æ±‚æŠ¥å‘Šï¼š')) {
          setTimeout(async () => {
            const globalProvider = settingsStore.getCurrentProvider()
            const globalModel = settingsStore.getCurrentModel()
            if (globalProvider && globalModel) {
              await generatePrompt(globalProvider, globalModel.id)
            }
          }, 800)
        }
      } else {
        const validMessages = promptStore.getValidMessages()
        const conversationHistory = validMessages.map(msg => ({
          type: msg.type,
          content: msg.content,
          attachments: msg.attachments || []
        }))
        const aiResponse = await aiGuideService.generateSimpleResponse(
          '',
          conversationHistory,
          provider,
          model.id,
          useStreamMode
        )

        const shouldEndConversation = checkAIDecision(aiResponse)
        
        if (shouldEndConversation || aiResponse.includes('åŸºäºæˆ‘ä»¬çš„å¯¹è¯ï¼Œæˆ‘ç°åœ¨ä¸ºæ‚¨ç”Ÿæˆéœ€æ±‚æŠ¥å‘Šï¼š')) {
          const cleanResponse = cleanAIResponse(aiResponse)
          await chatMessages.simulateTyping(cleanResponse, false)
          
          setTimeout(async () => {
            const globalProvider = settingsStore.getCurrentProvider()
            const globalModel = settingsStore.getCurrentModel()
            if (globalProvider && globalModel) {
              await generatePrompt(globalProvider, globalModel.id)
            }
          }, 800)
        } else {
          const cleanResponse = cleanAIResponse(aiResponse)
          await chatMessages.simulateTyping(cleanResponse, false)
        }
      }
    } catch (error: unknown) {
      promptStore.isTyping = false
      promptStore.isGenerating = false
      const errorMessage = error instanceof Error ? error.message : String(error)
      notificationStore.error(`é‡æ–°å‘é€å¤±è´¥: ${errorMessage}`)
      
      if (chatModel.isStreamMode.value) {
        const aiService = AIService.getInstance()
        aiService.clearStreamUpdateCallback()
      }
    }
  }

  return {
    chatContainerMaxHeight,
    initializeChat,
    clearChat,
    sendMessage,
    generatePrompt,
    regenerateMessage,
    resendUserMessage
  }
}
